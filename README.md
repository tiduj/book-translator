# ğŸ“š Book Translator

AI-powered translation system that actually understands context. Built for books, documents, and anything that needs more than word-by-word translation.

## ğŸ¯ How It Works

```
Original Text
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: Primary Translation       â”‚
â”‚  â€¢ Full context awareness           â”‚
â”‚  â€¢ Genre-specific adaptation        â”‚
â”‚  â€¢ Idiom localization              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: Self-Reflection          â”‚
â”‚  â€¢ AI critiques its own work       â”‚
â”‚  â€¢ Checks accuracy & naturalness   â”‚
â”‚  â€¢ Polishes final output           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
Final Translation
```

**No Google Translate. No DeepL. Pure LLM pipeline.**

## âš¡ Quick Start

```bash
# 1. Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. Pull a model
ollama pull llama3

# 3. Clone & run
git clone <repo-url>
cd BTrans
pip install -r requirements.txt
python translator.py
```

Open **http://localhost:5001** â†’ paste text â†’ translate

## ğŸ¨ Features

| Feature | Description |
|---------|-------------|
| **Smart Chunking** | Splits by paragraphs, not arbitrary limits |
| **Context Memory** | Each chunk knows what came before |
| **Genre Modes** | Fiction, technical, academic, business, poetry |
| **Two-Stage Quality** | Draft â†’ Reflection â†’ Final |
| **Multi-Format Export** | TXT, PDF, EPUB, clipboard |
| **Translation History** | Auto-saved, searchable, re-exportable |
| **Progress Tracking** | Real-time updates, 3-panel view |

## ğŸ”§ Supported Languages

English â€¢ Russian â€¢ Spanish â€¢ French â€¢ German â€¢ Italian â€¢ Portuguese â€¢ Chinese â€¢ Japanese â€¢ Korean

## ğŸ’¡ Example

**Input (English):**
```
He kicked the bucket last night.
```

**Bad translation (literal):**
```
ĞĞ½ Ğ¿Ğ½ÑƒĞ» Ğ²ĞµĞ´Ñ€Ğ¾ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¹ Ğ½Ğ¾Ñ‡ÑŒÑ.
```

**Our translation (Stage 1 + 2):**
```
ĞĞ½ ÑƒĞ¼ĞµÑ€ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¹ Ğ½Ğ¾Ñ‡ÑŒÑ.
```

Stage 1 understands idiom. Stage 2 ensures naturalness.

## ğŸ›ï¸ Configuration

Edit `translator.py` or use defaults:

```python
PORT = 5001
CHUNK_SIZE = 1000  # characters
OLLAMA_URL = "http://localhost:11434"

# Temperature by genre
TEMPERATURES = {
    'fiction': 0.7,    # more creative
    'technical': 0.3,  # more precise
    'poetry': 0.8      # most creative
}
```

## ğŸ“ Project Structure

```
BTrans/
â”œâ”€â”€ translator.py          # Backend (Flask + Ollama)
â”œâ”€â”€ static/index.html      # Frontend (vanilla JS)
â”œâ”€â”€ requirements.txt       # 4 dependencies
â”œâ”€â”€ logs/                  # Rotation logs
â”œâ”€â”€ translations/          # Exported files
â””â”€â”€ cache.db              # SQLite cache
```

## ğŸ› Troubleshooting

**"Connection refused"**
```bash
ollama serve  # start Ollama
```

**"Model not found"**
```bash
ollama list           # check installed
ollama pull llama3    # install model
```

**Slow performance**
- Use smaller models: `llama3:8b` instead of `llama3:70b`
- Reduce chunk size: 500 chars instead of 1000
- Check CPU/RAM usage in UI

**Clear cache**
```bash
rm cache.db translations.db
```

## ğŸš€ Advanced Usage

**Custom model:**
```bash
ollama pull mistral
# Select "mistral" in UI dropdown
```

**Batch processing:**
```python
# API endpoint
POST /api/translate
{
  "text": "...",
  "source_lang": "en",
  "target_lang": "ru",
  "genre": "fiction",
  "model": "llama3"
}
```

**Export formats:**
- TXT: plain text
- PDF: formatted with metadata
- EPUB: e-reader compatible
- Clipboard: instant copy

## ğŸ“Š Performance

| Model | Speed | Quality | RAM |
|-------|-------|---------|-----|
| llama3:8b | Fast | Good | 8GB |
| llama3:70b | Slow | Excellent | 64GB |
| mistral:7b | Fast | Good | 8GB |
| gemma:7b | Fast | Good | 8GB |

## ğŸ¤ Contributing

Found a bug? Have an idea? PRs welcome.

## ğŸ“„ License

MIT - do whatever you want

---

**Why this exists:** Machine translation is fast but dumb. This is slower but actually understands what it's translating.
